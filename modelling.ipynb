{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a396ef9-3e1f-410b-8b00-cc2df80bad52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaaa79d3-7eee-4bb5-95c0-6429a952296a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, when, isnan, regexp_extract\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Databricks automatically creates a 'spark' session.\n",
    "# If running locally, uncomment: spark = SparkSession.builder.appName(\"TitanicML\").getOrCreate()\n",
    "\n",
    "# Load the dataset (Assuming file is uploaded to DBFS or available at path)\n",
    "# You can upload titanic.csv to Databricks via 'Data' tab -> 'Create Table'\n",
    "file_path = \"/Volumes/workspace/default/titanic/titanic.csv\" \n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema and initial data\n",
    "df.printSchema()\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ea48a75-eb55-400b-8c23-e990e9481cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Data Cleaning and Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2922805b-a97c-48f5-9671-2cfb31f78a88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Cleaning and Handling Missing Values"
    }
   },
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "print(\"Missing values per column:\")\n",
    "\n",
    "# Define numeric columns for isnan()\n",
    "numeric_cols = [\"PassengerId\", \"Survived\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "\n",
    "missing_exprs = [\n",
    "    count(when((isnan(c) if c in numeric_cols else False) | col(c).isNull(), c)).alias(c)\n",
    "    for c in df.columns\n",
    "]\n",
    "df.select(missing_exprs).show()\n",
    "\n",
    "# --- Cleaning Strategy ---\n",
    "# 1. 'Age': Fill with Mean\n",
    "# 2. 'Embarked': Fill with Mode (usually 'S')\n",
    "# 3. 'Cabin': Too many missing, we often drop it or create a binary \"HasCabin\" feature (covered in Feature Engineering)\n",
    "\n",
    "# Impute Age with Mean\n",
    "imputer = Imputer(inputCols=[\"Age\"], outputCols=[\"Age\"]).setStrategy(\"mean\")\n",
    "df = imputer.fit(df).transform(df)\n",
    "\n",
    "# Fill Embarked with 'S' (most common) and Fare with 0 (if any nulls)\n",
    "df = df.fillna({'Embarked': 'S', 'Fare': 0})\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "missing_exprs_cleaned = [\n",
    "    count(when((isnan(c) if c in numeric_cols else False) | col(c).isNull(), c)).alias(c)\n",
    "    for c in df.columns\n",
    "]\n",
    "df.select(missing_exprs_cleaned).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b0c601e-a791-4447-be48-33b97a956553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d278d73-2f51-4fcb-987d-573c4ab15c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create \"FamilySize\" (SibSp + Parch + 1)\n",
    "df = df.withColumn(\"FamilySize\", col(\"SibSp\") + col(\"Parch\") + 1)\n",
    "\n",
    "# Create \"IsAlone\" (1 if FamilySize == 1, else 0)\n",
    "df = df.withColumn(\"IsAlone\", when(col(\"FamilySize\") == 1, 1).otherwise(0))\n",
    "\n",
    "# Extract Titles from Name (e.g., Mr, Mrs, Miss)\n",
    "df = df.withColumn(\"Title\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1))\n",
    "\n",
    "# Group rare titles into \"Rare\"\n",
    "rare_titles = [\"Dr\", \"Rev\", \"Major\", \"Col\", \"Mlle\", \"Mme\", \"Ms\", \"Capt\", \"Lady\", \"Jonkheer\", \"Don\", \"Countess\", \"Sir\"]\n",
    "df = df.withColumn(\"Title\", when(col(\"Title\").isin(rare_titles), \"Rare\").otherwise(col(\"Title\")))\n",
    "\n",
    "print(\"Data Cleaning and Engineering Completed in Spark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "626aee35-40ce-45a2-8eaf-f95d3296c9fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Categorical Encoding & Vector Assembly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc25bc9-9f70-48f9-a063-6c71a628c4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"Sex\", \"Embarked\", \"Title\"]\n",
    "\n",
    "# Create a list of indexers\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Index\") for c in cat_cols]\n",
    "\n",
    "# Run the indexers using a Pipeline (cleaner way to run multiple steps)\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_encoded = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Select only the numerical columns we need for Scikit-Learn\n",
    "final_cols = [\"Survived\", \"Pclass\", \"Age\", \"Fare\", \"FamilySize\", \"IsAlone\", \"Sex_Index\", \"Embarked_Index\", \"Title_Index\"]\n",
    "df_final = df_encoded.select(final_cols)\n",
    "\n",
    "display(df_final.limit(5))\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "pdf = df_final.toPandas()\n",
    "\n",
    "# Check the data types to ensure they are float/int suitable for sklearn\n",
    "print(pdf.info())\n",
    "\n",
    "# Separate X (Features) and y (Target)\n",
    "X = pdf.drop(\"Survived\", axis=1)\n",
    "y = pdf[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ae95adb-b8e8-459a-b7cf-7b5085de3845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Train/Test Split & Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70145ab0-df2b-46d1-bd91-12146ac61ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split Data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model Trained using Scikit-Learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dfbeb43-4808-457a-ad0c-5fa88d1bbc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7c5c04-8aa1-48d7-8455-4ec93367eaad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {acc:.2f}\")\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show() # In Databricks, this will render the plot inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77e5c68-a89e-4cbe-9b4d-60363eea4f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    " \n",
    "db_path = \"/Volumes/workspace/digital_twin/digital-twin/digital_twin.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "tables_df = spark.createDataFrame(conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall(), [\"table_name\"])\n",
    "display(tables_df)\n",
    "conn.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bfaeabf-0f6f-496b-851b-26355e387e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
